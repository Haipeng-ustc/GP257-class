{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b83b7a",
   "metadata": {},
   "source": [
    "\n",
    "# GPU Numba and CuPy Parallelization of Matrix Multiplication \n",
    "\n",
    "Similary to the multicore parallelization lab, in this lab we will be using Numba and CuPy to accelerate matrix-matrix multiplications using GPU. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba for CUDA](https://numba.readthedocs.io/en/stable/cuda/index.html)\n",
    "* [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb)\n",
    "* [Numba.CUDA by Graham Markell](https://github.com/numba/nvidia-cuda-tutorial)\n",
    "* [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/)\n",
    "* [CuPy Basics](https://docs.cupy.dev/en/stable/user_guide/basic.html)\n",
    "\n",
    "[//]: <> (GEOPHYS 257 Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff49d1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "* You need to request a *T4* node on the cluster. Don't forget that you need to add **--gres=gpu** to your srun command.\n",
    "* Reminder: on the *T4* nodes you need to load a different version of Python:\n",
    "```bash\n",
    "spack load python@3.10.7\n",
    "```\n",
    "\n",
    "* Import every Python module, object, and/or function that you need below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991c4400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env\n",
    "# srun -p T4 --gres=gpu:1 --pty bash --x11\n",
    "srun --partition=t4 --gres=gpu:1 --x11 --exclusive --pty /bin/bash\n",
    "\n",
    ". /home/spack/spack/share/spack/setup-env.sh\n",
    "spack load gcc@9.5.0\n",
    "spack compiler find\n",
    "spack load python@3.10.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e9a09",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 1: Matrix Transpose\n",
    "\n",
    "Before we examine matrix-matrix multiplication, we will first write a GPU kernel that transposes a square matrix.  This type of problem is a good introduction into how to use the CUDA threading model. The task for this exercise is to write a Numba CUDA kernel that will transpose a square matrix. \n",
    "\n",
    "**Before you start**, take a look at the following:\n",
    "* Read over the following notebook that explanes Numba.CUDA kernels: [Writing Numba.CUDA kernels Notebook](https://github.com/ContinuumIO/gtc2017-numba/blob/master/4%20-%20Writing%20CUDA%20Kernels.ipynb) \n",
    "* The first matrix-matrix multiplication code (the one that **doesn't** use shared memory) shown at [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). Understanding this code should give a pretty good idea on how to write the transpose kernel. The matrix-matrix kernel code from the NYU lab is shown below.\n",
    "```python\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "```\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Write a Numba.CUDA kernel that transpose an $NxN$ square matrix.\n",
    "* Be sure that the transpose kernel can transpose square matrices with sizes of $N$ as small as $N=2$ and as large as $N=10240$.\n",
    "* Using shared memory is **not** required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3f5c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, jit\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def mattran(A, B):\n",
    "    \"\"\" Perform the matrix transpose of a NxN square matrix, i.e., A = B^T\n",
    "    \"\"\"\n",
    "    \n",
    "    # obatin the index\n",
    "    xpos, ypos = cuda.grid(2)\n",
    "    \n",
    "    # perform the transpose\n",
    "    if xpos < A.shape[0] and ypos < A.shape[1]:\n",
    "        B[ypos, xpos] = A[xpos, ypos] \n",
    "\n",
    "\n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "N = 1024\n",
    "A = np.linspace(0, N*N,num=N*N, endpoint=True).reshape(N, N)\n",
    "d_A = cuda.to_device(A)\n",
    "d_B = cuda.device_array((N, N))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(np.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(np.ceil(A.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "mattran[blockspergrid, threadsperblock](d_A, d_B)\n",
    "\n",
    "# Copy the result back to the host\n",
    "B = d_B.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d022b6-97ba-48e7-8c67-8c8071cefad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.00000095e+00 2.00000191e+00 ... 1.02100097e+03\n",
      "  1.02200097e+03 1.02300098e+03]\n",
      " [1.02400098e+03 1.02500098e+03 1.02600098e+03 ... 2.04500195e+03\n",
      "  2.04600195e+03 2.04700195e+03]\n",
      " [2.04800195e+03 2.04900195e+03 2.05000196e+03 ... 3.06900293e+03\n",
      "  3.07000293e+03 3.07100293e+03]\n",
      " ...\n",
      " [1.04550500e+06 1.04550600e+06 1.04550700e+06 ... 1.04652600e+06\n",
      "  1.04652700e+06 1.04652800e+06]\n",
      " [1.04652900e+06 1.04653000e+06 1.04653100e+06 ... 1.04755000e+06\n",
      "  1.04755100e+06 1.04755200e+06]\n",
      " [1.04755300e+06 1.04755400e+06 1.04755500e+06 ... 1.04857400e+06\n",
      "  1.04857500e+06 1.04857600e+06]]\n",
      "\n",
      "\n",
      "[[0.00000000e+00 1.02400098e+03 2.04800195e+03 ... 1.04550500e+06\n",
      "  1.04652900e+06 1.04755300e+06]\n",
      " [1.00000095e+00 1.02500098e+03 2.04900195e+03 ... 1.04550600e+06\n",
      "  1.04653000e+06 1.04755400e+06]\n",
      " [2.00000191e+00 1.02600098e+03 2.05000196e+03 ... 1.04550700e+06\n",
      "  1.04653100e+06 1.04755500e+06]\n",
      " ...\n",
      " [1.02100097e+03 2.04500195e+03 3.06900293e+03 ... 1.04652600e+06\n",
      "  1.04755000e+06 1.04857400e+06]\n",
      " [1.02200097e+03 2.04600195e+03 3.07000293e+03 ... 1.04652700e+06\n",
      "  1.04755100e+06 1.04857500e+06]\n",
      " [1.02300098e+03 2.04700195e+03 3.07100293e+03 ... 1.04652800e+06\n",
      "  1.04755200e+06 1.04857600e+06]]\n"
     ]
    }
   ],
   "source": [
    "print(A)\n",
    "print('\\n')\n",
    "print(B)\n",
    "assert np.any(A == B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12070622",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 2: Using Numba CUDA to parallelize matrix multiplication: \n",
    "\n",
    "For this exercise, we will use Numba compiled GPU kernels that calculate matrix-matrix multiplication for square matrices. In particular, we will use a GPU kernel that doesn't used shared memory and compared to a GPU kernel that does use shared-memory. Please use the two kernel codes discussed in the following lab: [NYU Numba CUDA Lab5](https://nyu-cds.github.io/python-numba/05-cuda/). As you will see in this exercise, learning to use shared-memory (akin to user-controlled cache), can take a lot of practice, so in the next exercise, we examine how well the simple shared-memory kernel from the NYU lab compares to the optimized codes provided by NVIDIA in the CuPy package. \n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Copy the matrix-matrix kernel codes from the NYU lab. Test them for accurracy against *numpy.dot()* and also compare time runtimes these GPU kernels the numpy.dot() function as well. **Note:** Use [CUDA events](https://numba.readthedocs.io/en/stable/cuda-reference/host.html#events) when timing GPU kernel calls because the driver does not \"block\" the calling process (for case this is IPython). Insted, the kernel is sent to the GPU to run, and then the process (IPython) immediately continues to it's next bit of code. Contrary to GPU kernel calls, calls to copy data to or from the GPU will block the process. For these cases, the calls can be timed the same way that other Python calls are timed.<br> **For both GPU kernels:**\n",
    "    - Test with square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 5120$, $N=10240$, and $N=20480$. **Tip**, first make sure you can get the GPU codes to work and that you get correct results by testing with $N_{test}=32$.\n",
    "    - For each $N$ above, test the multiplication for both dtypes: *dtype=float32* and *dtype=float64*.\n",
    "    - Calculate and show the error between your functions and the *numpy.dot()* function. \n",
    "    - Calculate and show the *speedup* (or *slowdown*) of your GPU kernel for each $N$ vs *numpy.dot()*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\n",
    "    - For each $N$ vs, calculate and show the *speedup* of your GPU kernel using *dtype=float32* vs *dtype=float64*. Be sure to include the array copy times in the \"total-gpu-kernel runtime.\"\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "h_A = np.random.random((N, N)).astype(np.<float-type>)\n",
    "h_B = np.random.random((N, N)).astype(np.<float-type>)\n",
    "```    \n",
    "<br>\n",
    "\n",
    "3. For the device memory:\n",
    "    - Create **d_A** and **d_B** by copying **h_A** and **h_B** to the GPU, and be sure to time the copies\n",
    "    - Create **d_C** as device-array that is allocated on the GPU (device) only, and not on the host (**Do Not Copy**)\n",
    "    \n",
    "<br>\n",
    "\n",
    "4. After the GPU matrix-matrix multiplication kernel finishes, **copy** the the *device-array* **d_C** to the *host-array* **h_C**, and be sure to time this copy.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the speedup or slowdowns vs numpy as well as float32 vs float64. Remember, that your runtime for the GPU kernel include time to compile the kernel (not much you can do to control this). Futhermore, becasue you have to copy data to and off of the GPU, these copy times should be included in the \"total-gpu-kernel runtime.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99735477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda, float32, float64\n",
    "from timeit import default_timer as timer\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul_float32(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp\n",
    "    \n",
    "    \n",
    "@cuda.jit\n",
    "def fast_matmul_float64(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B\n",
    "    Each thread computes one element of the result matrix C\n",
    "    \"\"\"\n",
    "\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(int(A.shape[1] / TPB)):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e78958-78fd-455c-ba10-3d6ea64e22a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: N = 5120, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 1104.032959 ms\n",
      "  Numpy time: 240.688116 ms\n",
      "  Speed up  : 0.218008\n",
      "  Rela error: 8.205821e-08\n",
      "\n",
      "\n",
      "Case: N = 5120, dtype = <class 'numpy.float64'>:\n",
      "  GPU   time: 1694.944580 ms\n",
      "  Numpy time: 389.132014 ms\n",
      "  Speed up  : 0.229584\n",
      "  Rela error: 1.650166e-15\n",
      "\n",
      "\n",
      "Case: N = 10240, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 5840.136230 ms\n",
      "  Numpy time: 1404.815898 ms\n",
      "  Speed up  : 0.240545\n",
      "  Rela error: 8.061929e-08\n",
      "\n",
      "\n",
      "Case: N = 10240, dtype = <class 'numpy.float64'>:\n",
      "  GPU   time: 11165.965820 ms\n",
      "  Numpy time: 2539.452701 ms\n",
      "  Speed up  : 0.227428\n",
      "  Rela error: 2.326184e-15\n",
      "\n",
      "\n",
      "Case: N = 20480, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 44746.722656 ms\n",
      "  Numpy time: 8447.079863 ms\n",
      "  Speed up  : 0.188775\n",
      "  Rela error: 9.866385e-08\n",
      "\n",
      "\n",
      "Case: N = 20480, dtype = <class 'numpy.float64'>:\n",
      "  GPU   time: 86922.656250 ms\n",
      "  Numpy time: 21779.138505 ms\n",
      "  Speed up  : 0.250558\n",
      "  Rela error: 3.287642e-15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "for N in [5120, 10240, 20480]:\n",
    "    for dtype in [np.float32, np.float64]:\n",
    "        \n",
    "        # Initialize the data arrays\n",
    "        h_A = np.random.random((N, N)).astype(dtype)\n",
    "        h_B = np.random.random((N, N)).astype(dtype)\n",
    "        \n",
    "        # Create cuda events\n",
    "        evt_beg = cuda.event()\n",
    "        evt_end = cuda.event()\n",
    "\n",
    "        # Begin recording the time \n",
    "        evt_beg.record()\n",
    "\n",
    "        # Copy the arrays to the device\n",
    "        d_A = cuda.to_device(h_A)\n",
    "        d_B = cuda.to_device(h_B)\n",
    "        d_C = cuda.device_array((N, N))\n",
    "\n",
    "        # Configure the blocks\n",
    "        threadsperblock = (16, 16)\n",
    "        blockspergrid_x = int(np.ceil(h_A.shape[0] / threadsperblock[0]))\n",
    "        blockspergrid_y = int(np.ceil(h_B.shape[1] / threadsperblock[1]))\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "        # Start the kernel \n",
    "        matmul[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
    "\n",
    "        # Copy the result back to the host\n",
    "        h_C = d_C.copy_to_host()\n",
    "\n",
    "        # End recording the time \n",
    "        evt_end.record()\n",
    "        evt_end.synchronize()\n",
    "\n",
    "        # Perform Numpy solution \n",
    "        np_beg = timer()\n",
    "        np_C = np.dot(h_A, h_B)\n",
    "        np_end = timer()\n",
    "\n",
    "        # Print information\n",
    "        time_gpu = evt_beg.elapsed_time(evt_end)\n",
    "        time_np = float(np_end - np_beg) * 1000\n",
    "\n",
    "        print('Case: N = {}, dtype = {}:'.format(N, dtype))\n",
    "        print('  GPU   time: {:f} ms'.format(time_gpu))\n",
    "        print('  Numpy time: {:f} ms'.format(time_np))\n",
    "        print('  Speed up  : {:f}'.format(time_np/time_gpu))\n",
    "        print('  Rela error: {:.6e}'.format(np.sum(abs(np_C - h_C))/np.sum(abs(np_C))))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082306f-5457-43c7-b81e-2ac8e72df182",
   "metadata": {},
   "source": [
    "**Response**: when N = 5120, the numpy.dot() function is faster than the GPU kernel for both the np.float32 and np.float64 data types. But when N increases to 10240 and 20480, the GPU kernel is faster than the numpy.dot() function. And the speedup is more significant when N is becoming large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665ff723-5047-43d4-b616-4272e51c675f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU   time: 948.466492 ms\n",
      "  Numpy time: 214.667926 ms\n",
      "  Speed up  : 0.226332\n",
      "  Rela error: 7.901321e-08\n",
      "\n",
      "\n",
      "  GPU   time: 2065.178223 ms\n",
      "  Numpy time: 359.335531 ms\n",
      "  Speed up  : 0.173997\n",
      "  Rela error: 1.650352e-15\n",
      "\n",
      "\n",
      "  GPU   time: 4056.052002 ms\n",
      "  Numpy time: 1216.757579 ms\n",
      "  Speed up  : 0.299986\n",
      "  Rela error: 7.756453e-08\n",
      "\n",
      "\n",
      "  GPU   time: 13147.020508 ms\n",
      "  Numpy time: 2591.687422 ms\n",
      "  Speed up  : 0.197131\n",
      "  Rela error: 2.326432e-15\n",
      "\n",
      "\n",
      "  GPU   time: 30855.353516 ms\n",
      "  Numpy time: 10527.211300 ms\n",
      "  Speed up  : 0.341179\n",
      "  Rela error: 9.618172e-08\n",
      "\n",
      "\n",
      "  GPU   time: 102773.500000 ms\n",
      "  Numpy time: 21981.716861 ms\n",
      "  Speed up  : 0.213885\n",
      "  Rela error: 3.287668e-15\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "for N in [5120, 10240, 20480]:\n",
    "    for dtype in [np.float32, np.float64]:\n",
    "        \n",
    "        # Initialize the data arrays\n",
    "        h_A = np.random.random((N, N)).astype(dtype)\n",
    "        h_B = np.random.random((N, N)).astype(dtype)\n",
    "        \n",
    "        # Create cuda events\n",
    "        evt_beg = cuda.event()\n",
    "        evt_end = cuda.event()\n",
    "\n",
    "        # Begin recording the time \n",
    "        evt_beg.record()\n",
    "\n",
    "        # Copy arrays to the device\n",
    "        d_A = cuda.to_device(h_A)\n",
    "        d_B = cuda.to_device(h_B)\n",
    "        d_C = cuda.device_array((N, N), dtype=dtype)\n",
    "\n",
    "        # Configure the blocks\n",
    "        threadsperblock = (TPB, TPB)\n",
    "        blockspergrid_x = int(np.ceil(h_A.shape[0] / threadsperblock[1]))\n",
    "        blockspergrid_y = int(np.ceil(h_B.shape[1] / threadsperblock[0]))\n",
    "        blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "        # Start the kernel based on data type\n",
    "        if dtype is np.float32:\n",
    "            fast_matmul_float32[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
    "        elif  dtype is np.float64:\n",
    "            fast_matmul_float64[blockspergrid, threadsperblock](d_A, d_B, d_C)\n",
    "        else:\n",
    "            raise ValueError(f'Not support dtype {dtype}')\n",
    "            \n",
    "        # Copy the result back to the host\n",
    "        h_C = d_C.copy_to_host()\n",
    "\n",
    "        # End recording the time \n",
    "        evt_end.record()\n",
    "        evt_end.synchronize()\n",
    "\n",
    "        # Perform Numpy solution \n",
    "        np_beg = timer()\n",
    "        np_C = np.dot(h_A, h_B)\n",
    "        np_end = timer()\n",
    "\n",
    "        # Print information\n",
    "        time_gpu = evt_beg.elapsed_time(evt_end)\n",
    "        time_np = float(np_end - np_beg) * 1000\n",
    "\n",
    "        print('Case: N = {}, dtype = {}:'.format(N, dtype, ))\n",
    "        print('  GPU   time: {:f} ms'.format(time_gpu))\n",
    "        print('  Numpy time: {:f} ms'.format(time_np))\n",
    "        print('  Speed up  : {:f}'.format(time_np/time_gpu))\n",
    "        print('  Rela error: {:.6e}'.format(np.sum(abs(np_C - h_C))/np.sum(abs(np_C))))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9cb6d0-67db-40cb-b987-fa5d8dac50d1",
   "metadata": {},
   "source": [
    "**Response**: when using the shared memory, the GPU kernel is faster than the numpy.dot() function in all cases, including N = 5120, 10240 and 20480, and for both date type of float32 and float64. Also, the speedup is more significant when N is becoming large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f35012",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise 3: CuPy \n",
    "\n",
    "For this exercise, we will repeat what we did in *Exercise 2*. However, we will use *CuPy* functions, which are similar to *Numpy* funcstions with some added functions for copying data to-the-device-from-the-host and to-the-host-from-the-device. By using CuPy, we can depend on code that has been optimized for the GPU by NVIDIA, and instead of tyring to optimize our matrix-matrix multiplication kernels, we can use a built-in function to calculate the multiplication instead (i.e. [cupy.dot()](https://docs.cupy.dev/en/stable/reference/generated/cupy.dot.html#cupy.dot)).\n",
    "\n",
    "**Tasks for this exercise:**\n",
    "* Same as those listed in *Exercise 2*, but compare *cupy.dot()* to *numpy.dot()*.\n",
    "* Also, reuse the host-arrays, *h_A* and *h_B* above. You will need to call the appropriate *CuPy* fuctions to copy these arrays to the GPU and to copy the result back to the host. You will **not** need to declare the deive-C array before calling *cupy.dot()* because the function will do it for you (like numpy does).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6103d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: N = 5120, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 669.494263 ms\n",
      "  Numpy time: 3.706200 ms\n",
      "  Speed up  : 0.005536\n",
      "  Rela error: 0.000000e+00\n",
      "\n",
      "\n",
      "Case: N = 5120, dtype = <class 'numpy.float64'>:\n",
      "  GPU   time: 42.227711 ms\n",
      "  Numpy time: 0.062447 ms\n",
      "  Speed up  : 0.001479\n",
      "  Rela error: 0.000000e+00\n",
      "\n",
      "\n",
      "Case: N = 10240, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 320.380920 ms\n",
      "  Numpy time: 1.013658 ms\n",
      "  Speed up  : 0.003164\n",
      "  Rela error: 0.000000e+00\n",
      "\n",
      "\n",
      "Case: N = 10240, dtype = <class 'numpy.float64'>:\n",
      "  GPU   time: 320.115723 ms\n",
      "  Numpy time: 0.106688 ms\n",
      "  Speed up  : 0.000333\n",
      "  Rela error: 0.000000e+00\n",
      "\n",
      "\n",
      "Case: N = 20480, dtype = <class 'numpy.float32'>:\n",
      "  GPU   time: 2596.072510 ms\n",
      "  Numpy time: 187.907822 ms\n",
      "  Speed up  : 0.072382\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 3,355,443,200 bytes (allocated so far: 14,260,633,600 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44939/2499578770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Numpy time: {:f} ms'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Speed up  : {:f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_np\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtime_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  Rela error: {:.6e}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_C\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcp_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__sub__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/_kernel.pyx\u001b[0m in \u001b[0;36mcupy._core._kernel._get_out_args_from_optionals\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_init\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base._init_fast\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mcupy/cuda/memory.pyx\u001b[0m in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Out of memory allocating 3,355,443,200 bytes (allocated so far: 14,260,633,600 bytes)."
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "import cupy as cp\n",
    "\n",
    "for N in [5120, 10240, 20480]:\n",
    "    for dtype in [np.float32, np.float64]:\n",
    "        \n",
    "        # Initialize the data arrays\n",
    "        h_A = cp.random.randn(N, N)\n",
    "        h_B = cp.random.randn(N, N)\n",
    "        \n",
    "        # Create cuda events\n",
    "        evt_beg = cuda.event()\n",
    "        evt_end = cuda.event()\n",
    "\n",
    "        # Begin recording the time \n",
    "        evt_beg.record()\n",
    "\n",
    "        # Multiply the two matrices using Cupy's dot() function\n",
    "        cp_C = cp.dot(h_A, h_B)\n",
    "\n",
    "        # End recording the time \n",
    "        evt_end.record()\n",
    "        evt_end.synchronize()\n",
    "\n",
    "        # Perform Numpy solution \n",
    "        np_beg = timer()\n",
    "        np_C = np.dot(h_A, h_B)\n",
    "        np_end = timer()\n",
    "\n",
    "        # Print information\n",
    "        time_gpu = evt_beg.elapsed_time(evt_end)\n",
    "        time_np = float(np_end - np_beg) * 1000\n",
    "\n",
    "        print('Case: N = {}, dtype = {}:'.format(N, dtype, ))\n",
    "        print('  GPU   time: {:f} ms'.format(time_gpu))\n",
    "        print('  Numpy time: {:f} ms'.format(time_np))\n",
    "        print('  Speed up  : {:f}'.format(time_np/time_gpu))\n",
    "        print('  Rela error: {:.6e}'.format(np.sum(abs(np_C - cp_C))/np.sum(abs(np_C))))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53d48f5-f8c0-47f2-8bc9-aba3557071bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 24 11:12:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    36W / 250W |  13323MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  Off  | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    23W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    24W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    27W / 250W |      4MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     44939      C   ...peng/anaconda3/bin/python    13319MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294e1dfb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Exercise $\\mathbf{\\pi}$: CuPy Interoperability\n",
    "\n",
    "Numba and CuPy device arrays (GPU arrays) can be accept each other's arrays. See [Interoperability](https://docs.cupy.dev/en/stable/user_guide/interoperability.html).\n",
    "\n",
    "**Tasks for this exercise**\n",
    "* Use the **device** arrays, **d_A** and **d_B**, that were created in *Exercise 2* to calculate the matrix-matrix multiplcation using *cupy.dot()*.\n",
    "* Verify that you get the same results as you did in *Exercise 3*.\n",
    "* You will need to \"wrap\" the device arrays before passing them to *cupy.dot()*. Read the *Interoperability* documentation linked above.\n",
    "    - Time how long it takes (runtime) to \"wrap\" these arrays.\n",
    "    - Compare this runtime to the runtime it took to create the device arrays in *Exercise 3*.\n",
    "    - Provide a quick comment your thoughts on the runtime differences compared above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dbb6d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case: N = 5120, dtype = <class 'numpy.float32'>:\n",
      "  Cuda time: 45.512715 ms\n",
      "  Cupy time: 3.446711 ms\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "import numpy as np\n",
    "from numba import cuda, float32, float64\n",
    "import cupy as cp\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "for N in [5120]: # , 10240, 20480\n",
    "    for dtype in [np.float32]: # , np.float64\n",
    "        \n",
    "        # Initialize the data arrays\n",
    "        h_A = np.random.random((N, N)).astype(dtype)\n",
    "        h_B = np.random.random((N, N)).astype(dtype)\n",
    "        \n",
    "        # Copy the arrays to the device\n",
    "        cuda_beg = timer()\n",
    "        d_A = cuda.to_device(h_A)\n",
    "        d_B = cuda.to_device(h_B)\n",
    "        d_C = cuda.device_array((N, N))\n",
    "        cuda_end = timer()\n",
    "\n",
    "        \n",
    "        cupy_beg =  timer()\n",
    "        cp_A = cp.asarray(d_A)\n",
    "        cp_B = cp.asarray(d_B)\n",
    "        cupy_end = timer()\n",
    "\n",
    "        cp_C = cp.dot(cp_A, cp_B)\n",
    "\n",
    "        # Perform Numpy solution \n",
    "        np_beg = timer()\n",
    "        np_C = np.dot(h_A, h_B)\n",
    "        np_end = timer()\n",
    "\n",
    "        # Print information\n",
    "        time_np   = float(np_beg - np_end) * 1000\n",
    "        time_cuda = float(cuda_end - cuda_beg) * 1000\n",
    "        time_cupy = float(cupy_end - cupy_beg) * 1000\n",
    "\n",
    "        print('Case: N = {}, dtype = {}:'.format(N, dtype, ))\n",
    "        print('  Cuda time: {:f} ms'.format(time_cuda))\n",
    "        print('  Cupy time: {:f} ms'.format(time_cupy))\n",
    "        # print('  Rela error: {:.6e}'.format(np.sum(abs(np_C - cp_C))/np.sum(abs(np_C))))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c26056-8ec8-4cc1-aa8a-2b29f6e4df5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
