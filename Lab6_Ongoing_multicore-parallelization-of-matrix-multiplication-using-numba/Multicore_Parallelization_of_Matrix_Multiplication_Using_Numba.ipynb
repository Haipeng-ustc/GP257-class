{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "533203a6",
   "metadata": {},
   "source": [
    "# GEOPHYS 257 (Winter 2023)\n",
    "[//]: <> (Notebook Author: Thomas Cullison, Stanford University, Jan. 2023)\n",
    "\n",
    "## Multicore Parallelization of Matrix Multiplication Using Numba\n",
    "\n",
    "In this lab we will be using Numba to accelerate matrix-matrix multiplications by exploiting parallelism. Even most laptops today have Multicore CPUs, where a *core* is a microprocessor, and each core is usually a copy of the each other core. Accelerating the marrix-matrix multiplication operation is a good analog to accelerating other types of operators and computationally intense kernels, codes, and algorithms. Furthermore, the structure of matricies makes matrix-matrix multiplication a good place start learning how to parallelize code.\n",
    "\n",
    "\n",
    "## External Resources\n",
    "If you have any question regarding some specific Python functionality you can consult the official [Python documenation](http://docs.python.org/3/).\n",
    "\n",
    "* [Numba](https://numba.readthedocs.io/en/stable/index.html): Documentation\n",
    "* [Numba in 30 min](https://youtu.be/DPyPmoeUdcE): Conference presentation video\n",
    "\n",
    "\n",
    "## Required Preperation\n",
    "Please watch the following videos before starting the lab (each is pretty short):\n",
    "* [Introduction to Parallel Computing](https://youtu.be/RNVIcm8-6RE)\n",
    "* [Amdahl's Law](https://youtu.be/Axx2xuB-Xuo)\n",
    "* [CPU Caching](https://youtu.be/KmairurdiaY)\n",
    "* [Pipelining](https://youtu.be/zPmfprtdzCE)\n",
    "* [Instruction Level Parallelism](https://youtu.be/ZoDUI3gTkDI)\n",
    "* [Introduction to SIMD](https://youtu.be/o_n4AKwdfiA)\n",
    "\n",
    "\n",
    "### Exercise 0\n",
    "\n",
    "Please run the following cells.  Examine the result of the example function that makes use of the *timer* wrapper. Also please use this timer wrapper (defined below) for all the exercises that follow.\n",
    "\n",
    "#### Load python modules (Note: you may need to install some Python packages for the modules below, e.g. Numba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9957106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from functools import wraps\n",
    "from time import time, sleep\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc2aac",
   "metadata": {},
   "source": [
    "#### Define a function-decorator for timing the runtime of your functions\n",
    "\n",
    "Below is some code that you can use to wrap your functions so that you can time them individually.  The function defined imeadiately below the *timer()* is an example of how to use the warpper. In the cell that follows, execute this example function. (Note, for this timer, we're are making use of something called *decorators*, but a discussion about this feature is outside the scope of this class.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd0a801",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# defining a function decorator for timing other functions\n",
    "def mytimer(func):\n",
    "    @wraps(func)\n",
    "    def wrap(*args, **kwargs):\n",
    "        t_start = time() # Students look here\n",
    "        result = func(*args, **kwargs)\n",
    "        t_end = time() # Students also look here. This is how you can time things inside functions/classes\n",
    "        print(f'Function: \\'{func.__name__}({kwargs})\\' executed in {(t_end-t_start):4.6f}s\\n')\n",
    "        return result\n",
    "    return wrap\n",
    "    \n",
    "\n",
    "# Example of how to use. NOTE the \"@mytimer\" stated just above the function definition\n",
    "@mytimer\n",
    "def example_sum_timer_wrap(N):\n",
    "    \"\"\" Sum the squares of the intergers, i, contained in [1-N] \"\"\"\n",
    "    return np.sum(np.arange(1,N+1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb7b5c",
   "metadata": {},
   "source": [
    "#### Run the example function\n",
    "\n",
    "Run the example function for each of the following instances of $N = 10^5, 10^6, 10^7, 10^8$. Please examine the results, in particular, how the runtime changes with respect to $N$.\n",
    "\n",
    "\n",
    "**Answer** the following questions in the markdown cell that follow the code. \n",
    "* For each factor-of-ten increase in $N$, roughly how much longer was the runtime of the function?\n",
    "* Does this slowdown in the runtime make sense? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa466992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'example_sum_timer_wrap({})' executed in 0.002737s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({})' executed in 0.032228s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({})' executed in 0.638612s\n",
      "\n",
      "Function: 'example_sum_timer_wrap({})' executed in 6.897071s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the example function above for each value of N (try making one-call first, then loop)\n",
    "N = [10e5, 10e6, 10e7, 10e8]\n",
    "res=[]\n",
    "for n in N:\n",
    "    res.append(example_sum_timer_wrap(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4c88",
   "metadata": {},
   "source": [
    "#### Your answer for Exerciese 0 questions here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfafb6f-d43d-47ec-ad6f-0700082f4487",
   "metadata": {},
   "source": [
    "**Response**: \n",
    "From N=10e5 to N=10e6, the time is ~11.8 times longer. \n",
    "\n",
    "From N=10e6 to N=10e7, the time is ~19.8 times longer.\n",
    "\n",
    "From N=10e7 to N=10e8, the time is ~10.9 times longer.\n",
    "\n",
    "In general, the slowdown in the runtime make sense to me. With the increase of the size of array, the computational time also increase almost linearly. But I can find that the increasing rate is not strictly linear to that of the data size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8161fc",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 1: Naive matrix multiplication: \n",
    "\n",
    "For this exercise, we will write our own matrx-matrix multiplication function.  We are going to be naive about it, so we want to loop over individual indices, instead of using slicing (in the next section, we will parallelize this code and we want to see how well Numba does at speeding up the naive code).\n",
    "\n",
    "#### Tasks for this exercise\n",
    "\n",
    "1. Write a function with that calculates matrix-matrix multiplication such that $C = A\\cdot B$, where $A$, $B$, and $C$ are 2D numpy arrays. Make the dtype for the $C$ matrix the same as the dtype for $A$. Below is a stencil you can start with. Test your results for accuracy and performance against the np.dot() function. To time the np.dot() function, you can wrap it in another function and use the *mytimer* wrapper; however, please copy the code for testing the A and B dimensions into your function wrapper for the np.dot() function.  This keeps the runtime comparison as a more \"apples-to-apples\" like comparison.\n",
    "\n",
    "```python\n",
    "@mytimer\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zeros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in range(<val>):\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Define your functions in the cell below.\n",
    "3. Test and compare the accuracy and runtime of your functions in the next cell.\n",
    "    - Test with non-square Matrices: $A \\in \\mathbb{R}^{N\\times K}$ and $B \\in \\mathbb{R}^{K \\times M}$. With $N = 64$, $K = 32$, and $M = 128$.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 64, 128,$ and $256$.\n",
    "    - Test the following three cases:\n",
    "        - Case-1. $A$ and $B$ **both** have dtype=np.**float32** (make sure that $C$ is also of dtype=np.**float32**)\n",
    "        - Case-2. $A$ has dtype=np.**float64**, but $B$ has dtype=np.**float32**\n",
    "        - Case-3. $A$ and $B$ **both** have dtype=np.**float64** (make sure that $C$ is also of dtype=np.**float64**)\n",
    "    - For all three case above:\n",
    "        - Calculate and show the error by computing the sum of the differnece between the $C$ matrices computed from numpy.dot() and your my_naive_matmul() functions. Assume that numpy.dot() is correct\n",
    "        - Calculate and show the *speedup* that the faster function has versus the slower function.\n",
    "        - Comment on the which of the three cases is fastest, and comment on what the speedup of the fastest case is and why it is the fastest case.\n",
    "4. Create your matrices using random numbers. An example is shown below (feel free to copy this).\n",
    "\n",
    "```python\n",
    "A = np.random.rand(N, K)\n",
    "B = np.random.rand(\"for-you-to-figure-out\")\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 1 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6404b75",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define your functions for Exercise 1 here.\n",
    "\n",
    "@mytimer\n",
    "def my_naive_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a naive matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            for k in range(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "@mytimer\n",
    "def my_numpy_matmul(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a numpy matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # perform numpy matrix-multiplication\n",
    "    C = np.dot(A, B)\n",
    "    \n",
    "    # return the 2D numpy array for C\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db232378",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 1 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea011152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'my_naive_matmul({})' executed in 0.153648s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.010054s\n",
      "\n",
      "Test 1: C1 = C2: True, relative error 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Test your code here for runtime and accuracy\n",
    "\n",
    "# Test 1\n",
    "N, K, M = 64, 32, 128\n",
    "A = np.random.rand(N, K)\n",
    "B = np.random.rand(K, M)\n",
    "\n",
    "C1 = my_naive_matmul(A, B)\n",
    "C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "# compare two results\n",
    "comp = np.any(np.isclose(C1, C2))\n",
    "err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "print(f'Test 1: C1 = C2: {comp}, relative error {err:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0bc2d26c-2a7e-4d8b-8218-5c399e58d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'my_naive_matmul({})' executed in 0.160706s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000288s\n",
      "\n",
      "Test 2: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 1.153297s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000288s\n",
      "\n",
      "Test 2: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 8.748025s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000385s\n",
      "\n",
      "Test 2: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "for i in [64, 128, 256]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K)\n",
    "    B = np.random.rand(K, M)\n",
    "\n",
    "    C1 = my_naive_matmul(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test 2: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd1a00f6-40c0-41a9-abee-a8284cf6aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'my_naive_matmul({})' executed in 8.785456s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000572s\n",
      "\n",
      "Test 3-Case1: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 9.322998s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000561s\n",
      "\n",
      "Test 3-Case2: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 9.263541s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000396s\n",
      "\n",
      "Test 3-Case3: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3\n",
    "for i in range(3):\n",
    "    N, K, M = 256, 256, 256\n",
    "    if i == 0:\n",
    "        A = np.random.rand(N, K).astype(np.float32)\n",
    "        B = np.random.rand(K, M).astype(np.float32)\n",
    "    elif i == 1:\n",
    "        A = np.random.rand(N, K).astype(np.float64)\n",
    "        B = np.random.rand(K, M).astype(np.float32)\n",
    "    else:\n",
    "        A = np.random.rand(N, K).astype(np.float64)\n",
    "        B = np.random.rand(K, M).astype(np.float64)\n",
    "        \n",
    "    C1 = my_naive_matmul(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test 3-Case{i+1}: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6ca79ad7-e968-4806-b644-9c96a7048cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed up for Test 3 case1  : 1.0000\n",
      "Speed up for Test 3 case2  : 0.9423\n",
      "Speed up for Test 3 case3  : 0.9484\n"
     ]
    }
   ],
   "source": [
    "# Here, I compare the speed up between my function in these three cases\n",
    "print(f'Speed up for Test 3 case1  : {8.785456/8.785456:.4f}')\n",
    "print(f'Speed up for Test 3 case2  : {8.785456/9.322998:.4f}')\n",
    "print(f'Speed up for Test 3 case3  : {8.785456/9.263541:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cfb38-292a-4caf-9697-4710a232f2bb",
   "metadata": {},
   "source": [
    "**Response**: In my tests, I found in case 1 of Test 3, i.e., when A and B are both in the type of float32, the function achieve the best speedup. I suspect the type of float32 involves less computational cost compared with that of float64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae756d65",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Exercise 2: Using Numba to speed up matrix multiplication: \n",
    "\n",
    "For this exercise, numba to speedup our matrx-matrix multiplication function below. However, there is an interesting twist to this exercise. We will write six versions of the naive function you wrote above. One function for each of the six possible permutations of three loops used to calculate the multplication (i.e. ijk, ikj, jik, etc.)\n",
    "\n",
    "#### The tasks for this exercise:\n",
    "1. Make six copies your code for the my_naive_matmul(), one copy for each of the possible permutations and define them in the cell bellow:\n",
    "    - One of these function will have the same loop order as the my_naive_matmul() function. \n",
    "    - However name these functions: numba_mul_\\<perm\\>(), where \\<perm\\> should be replaced by the specific loop order of the function.\n",
    "2. In the cell that follows your function definitions, test and compare the accuracy and runtime of your functions against the numpy.dot() function.\n",
    "    - Test with a square Matrices: $A,B \\in \\mathbb{R}^{N\\times N}$. For the cases when $N = 128, 256,$ and $512$.\n",
    "    - For each matrix set their dtype as: dtype=np.float64\n",
    "    - Calculate and show the error between your functions and the numpy.dot() function. (Same as in Exercise 1.)\n",
    "    - Calculate and show the *speedup* that the fastest function has versus the all the other functions.\n",
    "    - You should notice that one of your permutation functions is faster than the others. For this case show the following:\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has versus the my_naive_matmul().\n",
    "        - Calculate and show the *speedup* that the fastest permutation function has when $A$, $B$, and $C$ are all of dtype=np.float64 vs all are of dtype=np.float32.\n",
    "3. Create your matrices using random numbers. (Same as in Exercise 1.) \n",
    "4. For each function, you need to add a function decorator for numba. Numba will *JIT* the function (**J**ust **I**n **T**ime compilation). \n",
    "    - Use the flag to keep a \"cache\" of the compiled code (not to be confused with CPU cache). **Note**: to get accurate timings, you will need to run your tests **twice**, because during the first run, the code is compiled and this compile time will be included in your runtime. After the first run, the compiled binary will be stored, so consecutive runs will be faster.\n",
    "    - Use the flag to use *fast math*\n",
    "    - Use the flag to disable the Python Global Interpretor Lock (GIL).\n",
    "    - The code below should get you started, but it is incomplete.\n",
    "    \n",
    "```python   \n",
    "    \n",
    "@mytimer\n",
    "@numba.njit(<flagname_caching>=<flag>, <flagname_fast_math>=<flag>, <flagname_no_gil>=<flag>)\n",
    "def numba_mul_<perm>(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # 1. \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    \n",
    "    # 2. \n",
    "    # construnct a 2D numpy array for matrix C, that is filled with zerros \n",
    "    # and that has the appropriate dimensions such taht C=A*B is a valid \n",
    "    # equation and operation\n",
    "    \n",
    "    # 3\n",
    "    # Write three nested for loops, with indices, i,j,k to solve the matrix-multiplication\n",
    "    # e.g.\n",
    "    # for i in numba.prange(<val>): # !!!! LOOK HERE !!!!\n",
    "    #   for j in range(<val>):\n",
    "    #     for k in range(<val>):\n",
    "    #       C[<c_row_index>,<c_col_index>] += A[<a_row_index>,<a_col_index>]*B[<b_row_index>,<b_col_index>]\n",
    "    \n",
    "    # 4\n",
    "    # return the 2D numpy array for C\n",
    "    return C\n",
    "```\n",
    "    \n",
    "6. Discuss your results in the markdown cell that follows your codes include in your discussion remarks about the questions asked in the markdown cell.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Write function definitions for Exercise 2 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dc104d3",
   "metadata": {
    "code_folding": [],
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_ijk({})' executed in 0.288583s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 0.245121s\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 0.399968s\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 0.298105s\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 0.035863s\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 0.288398s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your functions for Exercise 2 here.\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_ijk(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for i in numba.prange(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            for k in range(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_ikj(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for i in numba.prange(C.shape[0]):\n",
    "        for k in range(A.shape[1]):\n",
    "            for j in range(C.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_jki(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for j in numba.prange(C.shape[1]):\n",
    "        for k in range(A.shape[1]):\n",
    "            for i in range(C.shape[0]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_jik(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for j in numba.prange(C.shape[1]):\n",
    "        for i in range(C.shape[0]):\n",
    "            for k in range(A.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_kij(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for k in numba.prange(A.shape[1]):\n",
    "        for i in range(C.shape[0]):\n",
    "            for j in range(C.shape[1]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "\n",
    "@mytimer\n",
    "@numba.njit(cache=True, fastmath=True, nogil=False)\n",
    "def numba_mul_kji(A:np.ndarray, B:np.ndarray):\n",
    "    \"\"\" This is a Numba accelerated matrix-multiplication function \"\"\" \n",
    "    \n",
    "    # check that A and B have appropriate dimensions for multiplication\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise Exception(\"Error: the A.shape[1] and B.shape[0] should be the same!\")\n",
    "    \n",
    "    # construnct a 2D numpy array for matrix C\n",
    "    C = np.zeros((A.shape[0], B.shape[1]), dtype = A.dtype)\n",
    "    \n",
    "    # matrix-multiplication\n",
    "    for k in numba.prange(A.shape[1]):\n",
    "        for j in range(C.shape[1]):\n",
    "            for i in range(C.shape[0]):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "                \n",
    "    return C\n",
    "\n",
    "# compile these functions\n",
    "C = numba_mul_ijk(A, B)\n",
    "C = numba_mul_ikj(A, B)\n",
    "C = numba_mul_jik(A, B)\n",
    "C = numba_mul_jki(A, B)\n",
    "C = numba_mul_kij(A, B)\n",
    "C = numba_mul_kji(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4327e73a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Write the test codes for Exercise 2 in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6d3d475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_ijk({})' executed in 0.009386s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000457s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 0.057755s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000701s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_ijk({})' executed in 0.333588s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002381s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test your code here for runtimes, accuracy, and speedups\n",
    "\n",
    "# Test ijk\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_ijk(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0ec2811-dd89-4996-be48-bdc7c2535098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_ikj({})' executed in 0.008627s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000327s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 0.025420s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000468s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 0.177599s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002155s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test ikj\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_ikj(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80abd428-cd60-4c6f-bdc8-927366e8e66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_jik({})' executed in 0.009032s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000360s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 0.051185s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000280s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_jik({})' executed in 0.350315s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002412s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test jik\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_jik(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d896cfa-db84-4b71-b2d9-35783e673b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_jki({})' executed in 0.014186s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000334s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 0.112609s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000286s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_jki({})' executed in 0.944292s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002501s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test jki\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_jki(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "269c959a-de42-4a27-bfeb-bd36d5c21579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_kij({})' executed in 0.009045s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000332s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 0.023949s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.012121s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_kij({})' executed in 0.185937s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002185s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test kij\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_kij(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ee162719-199b-4dae-b571-c147f77459b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_kji({})' executed in 0.014377s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000338s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 0.110080s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.000297s\n",
      "\n",
      "Test: C1 = C2: True, relative error -0.0000\n",
      "\n",
      "\n",
      "Function: 'numba_mul_kji({})' executed in 0.958013s\n",
      "\n",
      "Function: 'my_numpy_matmul({})' executed in 0.002221s\n",
      "\n",
      "Test: C1 = C2: True, relative error 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test kji\n",
    "for i in [128, 256, 512]:\n",
    "    N, K, M = i, i, i\n",
    "    A = np.random.rand(N, K).astype(np.float64)\n",
    "    B = np.random.rand(K, M).astype(np.float64)\n",
    "\n",
    "    C1 = numba_mul_kji(A, B)\n",
    "    C2 = my_numpy_matmul(A, B)\n",
    "\n",
    "    # compare two results\n",
    "    comp = np.any(np.isclose(C1, C2))\n",
    "    err  = np.sum(C1 - C2) / np.sum(C2)\n",
    "\n",
    "    print(f'Test: C1 = C2: {comp}, relative error {err:.4f}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3889519e-32cd-4833-97b8-1677fa045aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed up for permutation ijk N = 128 : 0.9191\n",
      "Speed up for permutation ikj N = 128 : 1.0000\n",
      "Speed up for permutation jik N = 128 : 0.9552\n",
      "Speed up for permutation jki N = 128 : 0.6081\n",
      "Speed up for permutation kij N = 128 : 0.9538\n",
      "Speed up for permutation kji N = 128 : 0.6001\n",
      "\n",
      "\n",
      "Speed up for permutation ijk N = 256 : 0.4401\n",
      "Speed up for permutation ikj N = 256 : 1.0000\n",
      "Speed up for permutation jik N = 256 : 0.4966\n",
      "Speed up for permutation jki N = 256 : 0.2257\n",
      "Speed up for permutation kij N = 256 : 1.0614\n",
      "Speed up for permutation kji N = 256 : 0.2309\n",
      "\n",
      "\n",
      "Speed up for permutation ijk N = 512 : 0.5324\n",
      "Speed up for permutation ikj N = 512 : 1.0000\n",
      "Speed up for permutation jik N = 512 : 0.5070\n",
      "Speed up for permutation jki N = 512 : 0.1881\n",
      "Speed up for permutation kij N = 512 : 0.9552\n",
      "Speed up for permutation kji N = 512 : 0.1854\n"
     ]
    }
   ],
   "source": [
    "# Here, I only compare the speed up between different permutation schemes, ignoring the numpy dot \n",
    "print(f'Speed up for permutation ijk N = 128 : {0.008627/0.009386:.4f}')\n",
    "print(f'Speed up for permutation ikj N = 128 : {0.008627/0.008627:.4f}')\n",
    "print(f'Speed up for permutation jik N = 128 : {0.008627/0.009032:.4f}')\n",
    "print(f'Speed up for permutation jki N = 128 : {0.008627/0.014186:.4f}')\n",
    "print(f'Speed up for permutation kij N = 128 : {0.008627/0.009045:.4f}')\n",
    "print(f'Speed up for permutation kji N = 128 : {0.008627/0.014377:.4f}')\n",
    "print('\\n')\n",
    "print(f'Speed up for permutation ijk N = 256 : {0.025420/0.057755:.4f}')\n",
    "print(f'Speed up for permutation ikj N = 256 : {0.025420/0.025420:.4f}')\n",
    "print(f'Speed up for permutation jik N = 256 : {0.025420/0.051185:.4f}')\n",
    "print(f'Speed up for permutation jki N = 256 : {0.025420/0.112609:.4f}')\n",
    "print(f'Speed up for permutation kij N = 256 : {0.025420/0.023949:.4f}')\n",
    "print(f'Speed up for permutation kji N = 256 : {0.025420/0.110080:.4f}')\n",
    "print('\\n')\n",
    "print(f'Speed up for permutation ijk N = 512 : {0.177599/0.333588:.4f}')\n",
    "print(f'Speed up for permutation ikj N = 512 : {0.177599/0.177599:.4f}')\n",
    "print(f'Speed up for permutation jik N = 512 : {0.177599/0.350315:.4f}')\n",
    "print(f'Speed up for permutation jki N = 512 : {0.177599/0.944292:.4f}')\n",
    "print(f'Speed up for permutation kij N = 512 : {0.177599/0.185937:.4f}')\n",
    "print(f'Speed up for permutation kji N = 512 : {0.177599/0.958013:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc334918-f5a7-489d-a3ba-2eff47e030f7",
   "metadata": {},
   "source": [
    "**Response**: I notice that the 'ikj' permutation function is fater than the others. The following code block is for testing this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91115cf0-9f97-4022-97c5-e9f8c4b46ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: 'numba_mul_ikj({})' executed in 0.025298s\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 13.170306s\n",
      "\n",
      "Function: 'numba_mul_ikj({})' executed in 0.021406s\n",
      "\n",
      "Function: 'my_naive_matmul({})' executed in 12.628002s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 256\n",
    "# dtype=np.float64\n",
    "A = np.random.rand(N, N).astype(np.float64)\n",
    "B = np.random.rand(N, N).astype(np.float64)\n",
    "C1_float64 = numba_mul_ikj(A, B)\n",
    "C2_float64 = my_naive_matmul(A, B)\n",
    "\n",
    "# dtype=np.float32\n",
    "A = np.random.rand(N, N).astype(np.float32)\n",
    "B = np.random.rand(N, N).astype(np.float32)\n",
    "C1_float32 = numba_mul_ikj(A, B)\n",
    "C2_float32 = my_naive_matmul(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6778e7e-7f8b-4262-8ad4-d0ba26c98ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For N=256, dtype=np.float64, the speedup that the fastest permutation function has versus the my_naive_matmul() is 520.6066\n",
      "For N=256, dtype=np.float32, the speedup that the fastest permutation function has versus the my_naive_matmul() is 589.9282\n"
     ]
    }
   ],
   "source": [
    "print(f'For N={N}, dtype=np.float64, the speedup that the fastest permutation function has versus the my_naive_matmul() is {13.170306/0.025298:.4f}')\n",
    "print(f'For N={N}, dtype=np.float32, the speedup that the fastest permutation function has versus the my_naive_matmul() is {12.628002/0.021406:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c61afc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Discussion for Exercise 2\n",
    "1. What do you think is causing the differences in performance between the various permutations?\n",
    "1. Which function is fastest (permutations or numpy.dot)? Why do you think this function is the fastest, and could there be multiple factors involved regarding the superior performance?\n",
    "1. When comparing the matrix-matrix performance between the cases were all matrices had dtype=np.float64 vs.  all matrices having dtype=np.float32, which dtype was fastest? Roughtly what was the speedup when using this dtype vs the other?\n",
    "1. Does Amdahl's Law play a major factor in the performance differences? Which part of the matrix-matrix multiplication was not parallelized (serial portion)? (Hint: which matricies did we reuse for each function?) Could we parallelize this part, and if so, are there caveats?\n",
    "1. Do you think all codes should be parallelized? What about matrix-matrix multiplication? (This is a subjective question, but I am looking for a brief, but rational and informed arguement).\n",
    "1. For those taking the class for **4 units**: Regarding the performance differences, which Law do you think is more relevant when comparing the performance differences between each of the functions in this exercise, Amdahl's Law or Gustafson's Law? Explain your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33be8364-4a02-416e-af13-d3e704a6f90c",
   "metadata": {},
   "source": [
    "**Response 1**: Different permutations in Python matrix matrix multiplication can lead to different speed ups because the order in which the matrix multiplication is performed can affect the memory access patterns and the CPU cache utilization, which in turn can affect the overall performance. Different permutations of the multiplication operation may lead to better utilization of the CPU cache (reduce the number of cache misses), and hence better performance. Also, Python is an interpreted language, which means that it is generally slower than compiled languages like C or Fortran. This can lead to a significant performance difference for CPU-bound operations like matrix multiplication.\n",
    "\n",
    "**Response 2**: In my case, numpy.dot is the fastest because of fewer computational time compared with all permuttaions on testing case with N = 128, 256, and 512. The function of numpy.dot is faster than the one I implemented in Python because it is implemented in highly optimized C or Fortran code, and uses a variety of techniques to take advantage of the underlying hardware, such as CPU cache utilization and vectorization. In particular, NumPy uses the BLAS library, which provides highly optimized implementations of common linear algebra operations, including matrix multiplication.\n",
    "\n",
    "**Response 3**: In my case, I found when matrices both A and B both have dtype=np.float32 is faster than the case of dtype=np.float64. The reason for this is that dtype=np.float64 uses 64 bits of memory to represent each floating-point number, while dtype=np.float32 uses only 32 bits. This means that for a given matrix size, dtype=np.float32 matrices have half the memory footprint of dtype=np.float64 matrices, which can lead to better cache utilization and fewer cache misses. This can result in a significant performance improvement, especially on modern processors with larger cache sizes.\n",
    "\n",
    "**Response 4**: The Amdahl's Law dosen't play a major role in the performace differences. Amdahl's Law is a principle that states that the speedup of a parallel program is limited by the portion of the program that cannot be parallelized. In other words, if a program has a section that cannot be parallelized, the maximum speedup that can be achieved by parallelizing the program is limited by that section. However, in the case of matrix-matrix multiplication here, the parallelization strategy can have a significant impact on performance, but Amdahl's Law may not necessarily be the primary factor influencing the performance differences. The reason for this is that matrix-matrix multiplication is an inherently parallelizable operation, meaning that the majority of the computation can be split up and executed in parallel across multiple processing units. \n",
    "\n",
    "The serial portions that are not suitable for parallelization include the shape checking and array initilizating, which cannot be parallelized as far as I'm concerned.\n",
    "\n",
    "**Response 5**: I think not all codes can or should be parallelized, as there are some tasks that are inherently sequential or do not have enough parallelism to justify the overhead of parallelization. The decision to parallelize a code should be based on factors such as the task, the amount of parallelism that can be exploited and the expected performance gains. In the case of matrix-matrix multiplication, parallelization can often lead to significant performance improvements, especially for large matrices. This is because matrix-matrix multiplication involves a large number of mathematical operations that can be performed independently and in parallel across multiple processing units. \n",
    "\n",
    "**Response 6**: Amdahl's Law states that the speedup of a parallel program is limited by the portion of the program that cannot be parallelized. Gustafson's Law, on the other hand, states that the speedup of a parallel program can increase with the size of the problem. This law is relevant when comparing the performance differences between functions that scale differently with the size of the input data. In this case, Gustafson's Law can help to identify the function that has the highest potential speedup for larger input sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
